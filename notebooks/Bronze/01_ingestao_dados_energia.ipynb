{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a2c9e33-7bf2-49b4-afd5-6f9fd7167a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# C√©lula 1: Configura√ß√£o de Caminhos e Listas\n",
    "caminho_volume = \"/Volumes/catalogo_energia/bronze/dados_brutos_csv/\"\n",
    "\n",
    "print(f\"--> Configurando origem dos dados em: {caminho_volume}\")\n",
    "\n",
    "# Lista 1: Consumo Geral (Portugu√™s + Ingl√™s)\n",
    "arquivos_consumo_geral = [\n",
    "    f\"{caminho_volume}consumo_energia_2023.csv\",\n",
    "    f\"{caminho_volume}consumo_energia_2024.csv\",\n",
    "    f\"{caminho_volume}consumo_energia2025.csv\"\n",
    "]\n",
    "\n",
    "# Lista 2: Energia Limpa (Portugu√™s + Ingl√™s)\n",
    "arquivos_energia_limpa = [\n",
    "    f\"{caminho_volume}consumo_energia_limpa_2023.csv\",\n",
    "    f\"{caminho_volume}consumo_energia_limpa_2024.csv\",\n",
    "    f\"{caminho_volume}consumo_energia_limpa2025.csv\"\n",
    "]\n",
    "\n",
    "print(\"Listas de arquivos configuradas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ee99f7-99b4-4493-b520-ad62cd2aff9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# C√©lula 2: Defini√ß√£o da Fun√ß√£o de Ingest√£o (Vers√£o Higienizada)\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "def ingerir_para_bronze(lista_arquivos, nome_tabela_destino):\n",
    "    print(f\"--> Iniciando leitura e grava√ß√£o para: {nome_tabela_destino}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Leitura (Raw)\n",
    "        df = (spark.read\n",
    "              .format(\"csv\")\n",
    "              .option(\"header\", \"true\")\n",
    "              .option(\"inferSchema\", \"true\")\n",
    "              .load(lista_arquivos))\n",
    "        \n",
    "        # 1.5. Higieniza√ß√£o dos Nomes das Colunas\n",
    "        # Substitui espa√ßos por '_' e remove caracteres proibidos para o formato delta (: ,;{}()=)\n",
    "        novas_colunas = []\n",
    "        for coluna in df.columns:\n",
    "            nova_coluna = (coluna.replace(\" \", \"_\")\n",
    "                                 .replace(\",\", \"\")\n",
    "                                 .replace(\";\", \"\")\n",
    "                                 .replace(\"{\", \"\")\n",
    "                                 .replace(\"}\", \"\")\n",
    "                                 .replace(\"(\", \"\")\n",
    "                                 .replace(\")\", \"\")\n",
    "                                 .replace(\"\\n\", \"\")\n",
    "                                 .replace(\"\\t\", \"\")\n",
    "                                 .replace(\"=\", \"\"))\n",
    "            novas_colunas.append(nova_coluna)\n",
    "        \n",
    "        # Aplica os novos nomes ao DataFrame\n",
    "        df = df.toDF(*novas_colunas)\n",
    "        \n",
    "        # 2. Enriquecimento (Metadados)\n",
    "        df_enrich = (df\n",
    "                     .withColumn(\"nome_arquivo_origem\", col(\"_metadata.file_path\"))\n",
    "                     .withColumn(\"data_carga\", current_timestamp()))\n",
    "        \n",
    "        # 3. Escrita (Delta)\n",
    "        (df_enrich.write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"overwriteSchema\", \"true\")\n",
    "         .saveAsTable(f\"catalogo_energia.bronze.{nome_tabela_destino}\"))\n",
    "        \n",
    "        print(f\"‚úÖ Sucesso! Tabela criada: catalogo_energia.bronze.{nome_tabela_destino}\")\n",
    "        print(f\"üìä Total de linhas: {df_enrich.count()}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao processar {nome_tabela_destino}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5451a35c-aea0-4395-bd1a-5a1e632e41e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# C√©lula 3: Executando a carga\n",
    "ingerir_para_bronze(arquivos_consumo_geral, \"consumo_geral\")\n",
    "ingerir_para_bronze(arquivos_energia_limpa, \"energia_limpa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3770f760-6e35-4e40-971c-3dd268e05f1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingestao_dados_energia",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
